# --- START OF FILE configs/config_full_500m.yaml ---
# Full configuration for a ~500M parameter model.
# Enables ALL components: BLT, Titans, T2, MVoT.
# Requires setting paths for BLT/MVoT components if pretrained.

# Core Model Parameters (~500M target)
hidden_size: 2048
num_layers: 24
num_attention_heads: 16
intermediate_size: 8192 # 4 * hidden_size
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1
max_position_embeddings: 4096 # Needs to be large enough for max BLT patches or block_size
vocab_size: 260 # Set to byte vocab size when using BLT

# Component Activation Flags
use_blt_processor: true # Enable BLT
use_titans_memory: true # Enable Titans memory
use_transformer2_adaptation: true # Enable T2 adaptation
use_mvot_processor: true # Enable MVoT

# Nested Component Configurations
blt:
  entropy_threshold: 0.5
  min_patch_size: 8
  max_patch_size: 128
  num_local_layers: 1
  num_latent_layers: 2
  byte_lm:
    hidden_size: 256
    num_layers: 4
    num_attention_heads: 4
    intermediate_size: 1024
    byte_lm_dropout: 0.1
    byte_lm_model_type: "transformer"
  # !! IMPORTANT: Set path if using a pretrained SmallByteLM for entropy calculation
  blt_checkpoint_path: null # e.g., "./models/small_byte_lm.pt"

titans:
  use_window_attention: true
  use_surprise_based: true
  use_persistent: true
  window_size: 512
  memory_size: 4096
  surprise_threshold: 0.5
  num_persistent_vectors: 64
  persistent_init_scale: 0.02
  base_decay_rate: 0.99
  importance_half_life: 1000
  memory_prune_threshold: 0.01
  surprise_method: "gradient_norm"

transformer2:
  num_tasks: 8
  task_embedding_dim: 64
  num_singular_values: 128 # k
  expert_init_scale: 0.01
  adapt_attention: true
  adapt_ffn: true
  adapt_embeddings: false
  adapt_lm_head: false
  layer_specific: false
  use_randomized_svd: true
  svd_precision: "fixed"
  svd_n_oversamples: 10
  svd_n_iter: 5
  enable_svd_caching: true
  svd_cache_dir: ".svd_cache_full" # Use separate cache dir

mvot:
  codebook_size: 8192
  embedding_dim: 2048 # Match hidden_size for simplicity (no projection needed)
  discrepancy_loss_weight: 0.1
  codebook_model_type: "vqvae" # Hint for loading function
  # !! IMPORTANT: Set path to load pretrained visual codebook embeddings
  codebook_path: null # e.g., "./models/vqvae_codebook.safetensors"
  use_pretrained_codebook: true # Enable loading

# Data, Training, Hardware Configs
data:
  train_data_dir: "./data/processed/train" # Example path
  eval_data_dir: "./data/processed/eval"   # Example path
  # !! IMPORTANT: Change pattern if data is not raw text (e.g., binary chunks for BLT)
  train_file_pattern: "*.bin" # Assumes binary files if using BLT
  eval_file_pattern: "*.bin"
  block_size: 4096 # Sequence length in bytes for BLT

training:
  learning_rate: 3.0e-4
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0
  max_steps: 200000
  warmup_steps: 2000
  batch_size: 8
  gradient_accumulation_steps: 4
  logging_steps: 100
  eval_steps: 1000
  save_steps: 1000
  output_dir: "./output/neat_full_500m"
  resume_from: null

hardware:
  mixed_precision: true
  gradient_checkpointing: false
  force_cpu: false
  num_workers: 4
  use_flash_attention: true

# --- END OF FILE configs/config_full_500m.yaml ---