"""
Data loaders for mathematical problems for the NEAT architecture.

This module provides PyTorch-compatible data loaders for the mathematical
problems generated by the synthetic data generator.
"""

import torch
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional, Union, Callable
from torch.utils.data import DataLoader, Dataset

from src_OLD.data.generators.math_generator import (
    MathProblem,
    MathDataGenerator,
    NEATMathDataset,
    DifficultyLevel,
    ProblemType
)

logger = logging.getLogger(__name__)

class MathDataTokenizer:
    """
    Tokenizer for mathematical problem text.
    
    This is a simple tokenizer that converts math problems to token IDs
    suitable for processing by the NEAT architecture.
    """
    
    def __init__(self, vocab_size: int = 1000, max_length: int = 128):
        """
        Initialize the tokenizer.
        
        Args:
            vocab_size: Size of the vocabulary
            max_length: Maximum sequence length
        """
        self.vocab_size = vocab_size
        self.max_length = max_length
        
        # Create a simple vocabulary (this would be more sophisticated in a real implementation)
        self.token_to_id = {"<pad>": 0, "<unk>": 1, "<eos>": 2}
        self.id_to_token = {0: "<pad>", 1: "<unk>", 2: "<eos>"}
        
        # Add numbers to vocabulary
        for i in range(1000):  # Add numbers 0-999
            token = str(i)
            if token not in self.token_to_id:
                token_id = len(self.token_to_id)
                self.token_to_id[token] = token_id
                self.id_to_token[token_id] = token
                
        # Add common words and symbols for math problems
        common_tokens = [
            "+", "-", "Ã—", "*", "/", "=", "?",
            "what", "is", "the", "sum", "of", "and", "plus", "minus",
            "multiply", "multiplied", "by", "divide", "divided", "if", "you", "have",
            "many", "more", "less", "than", "difference", "between", "product",
            "pattern", "sequence", "next", "number", "comes", "following"
        ]
        
        for token in common_tokens:
            if token not in self.token_to_id:
                token_id = len(self.token_to_id)
                self.token_to_id[token] = token_id
                self.id_to_token[token_id] = token
                
        logger.info(f"Initialized tokenizer with {len(self.token_to_id)} tokens")
    
    def tokenize(self, text: str) -> List[int]:
        """
        Convert text to token IDs.
        
        Args:
            text: The input text to tokenize
            
        Returns:
            List of token IDs
        """
        # Simple whitespace tokenization (a real implementation would be more sophisticated)
        words = text.lower().split()
        tokens = []
        
        for word in words:
            # Handle punctuation by separating it
            word = word.strip(".,?!()[]{}\"'")
            
            # Add the word token
            if word in self.token_to_id:
                tokens.append(self.token_to_id[word])
            else:
                tokens.append(self.token_to_id["<unk>"])
        
        # Add end of sequence token
        tokens.append(self.token_to_id["<eos>"])
        
        # Truncate or pad to max_length
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length]
        else:
            tokens.extend([self.token_to_id["<pad>"]] * (self.max_length - len(tokens)))
        
        return tokens
    
    def encode(self, texts: List[str]) -> torch.Tensor:
        """
        Encode a batch of texts to token ID tensors.
        
        Args:
            texts: List of texts to encode
            
        Returns:
            Tensor of token IDs with shape (batch_size, max_length)
        """
        token_lists = [self.tokenize(text) for text in texts]
        return torch.tensor(token_lists)
    
    def decode(self, token_ids: torch.Tensor) -> List[str]:
        """
        Decode token IDs back to text.
        
        Args:
            token_ids: Tensor of token IDs
            
        Returns:
            List of decoded texts
        """
        if token_ids.dim() == 1:
            # Single sequence
            tokens = [self.id_to_token.get(int(idx), "<unk>") for idx in token_ids]
            # Remove padding and stop at EOS
            if "<eos>" in tokens:
                tokens = tokens[:tokens.index("<eos>")]
            while tokens and tokens[-1] == "<pad>":
                tokens.pop()
            return " ".join(tokens)
        else:
            # Batch of sequences
            return [self.decode(seq) for seq in token_ids]


class NEATMathDataLoader:
    """
    DataLoader for mathematical problems for the NEAT architecture.
    
    This class provides PyTorch-compatible data loaders with appropriate
    preprocessing for the NEAT architecture.
    """
    
    def __init__(self, 
                 tokenizer: Optional[MathDataTokenizer] = None,
                 batch_size: int = 32,
                 device: str = "cpu"):
        """
        Initialize the data loader.
        
        Args:
            tokenizer: Optional tokenizer for text processing
            batch_size: Batch size for training
            device: Device to place tensors on ("cpu" or "cuda")
        """
        self.generator = MathDataGenerator()
        self.tokenizer = tokenizer or MathDataTokenizer()
        self.batch_size = batch_size
        self.device = device
        
        logger.info(f"Initialized NEATMathDataLoader with batch size {batch_size}")
    
    def _prepare_batch(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """
        Prepare a batch for model training.
        
        Args:
            batch: List of examples
            
        Returns:
            Dictionary of tensors
        """
        questions = [example["question"] for example in batch]
        answers = [example["answer"] for example in batch]
        
        # Tokenize questions
        question_tensors = self.tokenizer.encode(questions)
        
        # Convert answers to tensors
        answer_tensors = torch.tensor([int(ans) for ans in answers])
        
        # Create difficulty and problem type tensors
        difficulty_tensors = torch.tensor([example["difficulty"] for example in batch])
        problem_type_tensors = torch.tensor([example["problem_type"] for example in batch])
        
        # Create attention masks (1 for actual tokens, 0 for padding)
        attention_mask = (question_tensors != self.tokenizer.token_to_id["<pad>"]).float()
        
        # Move to device
        question_tensors = question_tensors.to(self.device)
        answer_tensors = answer_tensors.to(self.device)
        difficulty_tensors = difficulty_tensors.to(self.device)
        problem_type_tensors = problem_type_tensors.to(self.device)
        attention_mask = attention_mask.to(self.device)
        
        return {
            "input_ids": question_tensors,
            "attention_mask": attention_mask,
            "labels": answer_tensors,
            "difficulty": difficulty_tensors,
            "problem_type": problem_type_tensors
        }
    
    def create_dataloader(self, 
                         dataset: NEATMathDataset, 
                         shuffle: bool = True) -> DataLoader:
        """
        Create a PyTorch DataLoader for a dataset.
        
        Args:
            dataset: The dataset to load
            shuffle: Whether to shuffle the data
            
        Returns:
            PyTorch DataLoader
        """
        return DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=shuffle,
            collate_fn=self._prepare_batch
        )
    
    def generate_train_eval_dataloaders(self, 
                                      train_size: int = 1000,
                                      eval_size: int = 200,
                                      include_train_difficulties: Optional[List[DifficultyLevel]] = None,
                                      include_eval_difficulties: Optional[List[DifficultyLevel]] = None) -> Tuple[DataLoader, DataLoader]:
        """
        Generate training and evaluation data loaders.
        
        Args:
            train_size: Size of the training dataset
            eval_size: Size of the evaluation dataset
            include_train_difficulties: Difficulty levels to include in training
            include_eval_difficulties: Difficulty levels to include in evaluation
            
        Returns:
            Tuple of (train_dataloader, eval_dataloader)
        """
        # Set default difficulties if not provided
        if include_train_difficulties is None:
            include_train_difficulties = [DifficultyLevel.BASIC, DifficultyLevel.MEDIUM]
        if include_eval_difficulties is None:
            include_eval_difficulties = [DifficultyLevel.BASIC, DifficultyLevel.MEDIUM, 
                                       DifficultyLevel.ADVANCED]
        
        logger.info(f"Generating train/eval dataloaders with {train_size} training and {eval_size} evaluation examples")
        
        # Generate train/test split
        train_problems, eval_problems = self.generator.generate_train_test_split(
            train_size=train_size,
            test_size=eval_size,
            train_difficulties=include_train_difficulties,
            test_difficulties=include_eval_difficulties
        )
        
        # Create datasets
        train_dataset = NEATMathDataset(train_problems, self.tokenizer)
        eval_dataset = NEATMathDataset(eval_problems, self.tokenizer)
        
        # Create dataloaders
        train_dataloader = self.create_dataloader(train_dataset, shuffle=True)
        eval_dataloader = self.create_dataloader(eval_dataset, shuffle=False)
        
        return train_dataloader, eval_dataloader
    
    def generate_progressive_dataloader(self, 
                                       base_size: int = 500,
                                       include_difficulties: Optional[List[DifficultyLevel]] = None,
                                       shuffle: bool = True) -> DataLoader:
        """
        Generate a dataloader with progressive difficulty levels.
        
        Args:
            base_size: Base number of problems per difficulty level
            include_difficulties: Difficulty levels to include
            shuffle: Whether to shuffle the data
            
        Returns:
            DataLoader for progressive difficulty training
        """
        if include_difficulties is None:
            include_difficulties = [DifficultyLevel.BASIC, DifficultyLevel.MEDIUM, 
                                  DifficultyLevel.ADVANCED]
            
        logger.info(f"Generating progressive dataloader across {len(include_difficulties)} difficulty levels")
        
        # Generate progressive dataset
        problems = self.generator.generate_progressive_dataset(
            base_size=base_size,
            include_difficulties=include_difficulties
        )
        
        # Create dataset and dataloader
        dataset = NEATMathDataset(problems, self.tokenizer)
        dataloader = self.create_dataloader(dataset, shuffle=shuffle)
        
        return dataloader


# Example usage:
if __name__ == "__main__":
    # Initialize tokenizer and data loader
    tokenizer = MathDataTokenizer()
    data_loader = NEATMathDataLoader(tokenizer)
    
    # Generate train/eval data loaders
    train_dataloader, eval_dataloader = data_loader.generate_train_eval_dataloaders(
        train_size=100,
        eval_size=20
    )
    
    # Print information about the data loaders
    print(f"Train dataloader has {len(train_dataloader)} batches")
    print(f"Eval dataloader has {len(eval_dataloader)} batches")
    
    # Get a batch from the training data loader
    batch = next(iter(train_dataloader))
    print("Batch keys:", batch.keys())
    print("Input IDs shape:", batch["input_ids"].shape)
    print("Labels shape:", batch["labels"].shape)
    
    # Decode a sample from the batch
    sample_idx = 0
    decoded_question = tokenizer.decode(batch["input_ids"][sample_idx])
    print("\nSample question:", decoded_question)
    print("Sample answer:", batch["labels"][sample_idx].item())
    print("Sample difficulty:", batch["difficulty"][sample_idx].item())
    print("Sample problem type:", batch["problem_type"][sample_idx].item())