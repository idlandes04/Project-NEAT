# Phase 3.1.1: Synthetic Data Generator Implementation Plan

## Overview

This document outlines the plan for Phase 3.1.1 of Project NEAT: the Synthetic Data Generator Integration. This phase involves developing a comprehensive data generation and loading system to support the evaluation of the NEAT architecture, particularly focusing on demonstrating the benefits of test-time learning and component-based architecture.

## Completed Tasks

1. **Basic Data Generation**
   - [x] Implemented `MathDataGenerator` class for generating diverse mathematical problems
   - [x] Added support for progressive difficulty levels (BASIC, MEDIUM, ADVANCED, COMPLEX)
   - [x] Implemented multiple problem types (addition, subtraction, multiplication, sequence)
   - [x] Created a robust template system for varied problem representations
   - [x] Added train/test split functionality with controlled distribution shifts

2. **Data Loading Infrastructure**
   - [x] Implemented `MathDataTokenizer` for converting problems to token IDs
   - [x] Created `NEATMathDataset` for PyTorch compatibility
   - [x] Developed `NEATMathDataLoader` with batch preparation
   - [x] Added utilities for train/eval split creation

3. **Testing & Validation**
   - [x] Created comprehensive test suite for the data generator
   - [x] Implemented test cases for the data loader
   - [x] Added a demo script to visualize generated problems
   - [x] Validated operations across different difficulty levels

## Completed Tasks (Phase 3.1.1)

1. **Advanced Problem Types**
   - [x] Implement multi-step reasoning problems
   - [x] Add algebraic equation problems for higher difficulty levels
   - [x] Create sequence problems with non-linear patterns
   - [x] Implement mixed-operation problems (e.g., nested arithmetic)

2. **Data Generation for Component Evaluation**
   - [x] Create targeted problem sets to evaluate Titans memory benefits
   - [x] Develop problems for evaluating Transformer² adaptation
   - [x] Implement visual math problems for MVoT testing
   - [x] Design problems for evaluating BLT byte-level processing

3. **Model Integration**
   - [x] Connect data generator to main training pipeline
   - [x] Extend tokenizer to integrate with all NEAT components
   - [x] Implement efficient data caching for faster iteration
   - [x] Create data augmentation strategies for robust training

4. **Evaluation Metrics**
   - [x] Design metrics for measuring generalization performance
   - [x] Implement metrics for memory utilization across problem types
   - [x] Create visualization tools for comparing model variations
   - [x] Design controlled difficulty progression for component evaluation

## Implementation Approach

1. **Progressive Complexity**
   - Start with simpler problem types and gradually extend to more complex ones
   - Ensure each component of the NEAT architecture can be evaluated individually
   - Create problem sets that target specific component benefits

2. **Component-Specific Testing**
   - For Titans Memory: Problems requiring long-term context retention
   - For Transformer² Adaptation: Problems with varied patterns requiring adaptation
   - For MVoT: Problems with visual/spatial elements that benefit from visualization
   - For BLT: Problems with varying entropy levels to test dynamic patching

3. **Controlled Distribution Shifts**
   - In-distribution testing: Problems similar to training set
   - Out-of-distribution: Problems with similar patterns but different value ranges
   - Cross-domain: Problems requiring transfer of concepts between domains

## Next Steps

All tasks for Phase 3.1.1 have been completed. We have:

1. Implemented advanced problem types (multi-step, algebraic, non-linear sequences)
2. Created component-specific problem sets for evaluating each NEAT component
3. Integrated the data generator with the main training pipeline
4. Implemented comprehensive evaluation metrics
5. Fixed all test failures and prepared for full-scale training

The project is now ready to proceed to Phase 3.1.2: Baseline Transformer Implementation, where we will:

1. Create parameter-matched baseline transformer models for comparative evaluation
2. Implement shared evaluation harness for consistent benchmarking
3. Develop the metrics to measure component-specific benefits
4. Train the full 100M parameter model on the Windows PC with 3080ti GPU

These steps will enable us to fully evaluate the benefits of the NEAT architecture compared to traditional models, particularly in terms of test-time learning, adaptation, and generalization capabilities.

## Dependencies

- PyTorch for data loading integration
- Core NEAT components for model-specific data preparation
- Test infrastructure for validation

# Phase 3.1.1: Synthetic Data Generator Implementation (Completed)

This document provides an overview of the implementation for Phase 3.1.1 of Project NEAT, which focuses on the synthetic data generator integration. All tests are now passing, and the system is ready for full-scale training on the Windows PC with 3080ti GPU.

## Implementation Overview

The synthetic data generation system is designed to provide diverse mathematical problems at various difficulty levels to evaluate the NEAT architecture's capabilities. It includes:

1. **Basic Data Generation**
   - Core `MathDataGenerator` class for generating mathematical problems
   - Support for multiple problem types and difficulty levels
   - Template-based generation system with varied problem representation

2. **Advanced Problem Types**
   - Multi-step reasoning problems requiring intermediate calculations
   - Algebraic equation problems for solving unknown variables
   - Non-linear sequences with quadratic, exponential, and Fibonacci patterns
   - Component-specific problems to test NEAT architecture features

3. **Component-Specific Problems**
   - **Titans Memory Test**: Problems designed to test long-term memory capabilities
   - **Transformer² Test**: Pattern adaptation problems for testing model adaptability
   - **MVoT-Compatible**: Problems that could benefit from visual thinking
   - **BLT-Friendly**: Problems with varying entropy levels for dynamic patching

4. **Data Loading Infrastructure**
   - PyTorch-compatible dataset and data loader implementations
   - Tokenization for converting text problems to model inputs
   - Efficient batching and preprocessing

## Directory Structure

```
project-neat/
├── src/
│   └── data/
│       ├── synthetic/
│       │   ├── __init__.py
│       │   └── math_generator.py   # Core generator implementation
│       └── loaders/
│           ├── __init__.py
│           └── math_data_loader.py  # PyTorch data loading utilities
├── scripts/
│   ├── download_training_data.py   # Script to download component training data
│   ├── prepare_training_dataset.py # Script to create training datasets
│   ├── test_advanced_problems.py   # Script to test problem generation
│   └── train_neat_model.sh         # End-to-end training script
└── data/
    ├── byte_training/              # Training data for BLT entropy estimator
    ├── byte_eval/                  # Evaluation data for BLT entropy estimator
    ├── visual_training/            # Mock visual codebook for MVoT
    └── neat_training/              # Generated datasets for NEAT model training
```

## Usage

### Generating Problems

```python
from src.data.synthetic.math_generator import MathDataGenerator, DifficultyLevel, ProblemType

# Initialize generator
generator = MathDataGenerator()

# Generate a basic addition problem
problem = generator.generate_problem(
    difficulty=DifficultyLevel.BASIC,
    problem_type=ProblemType.ADDITION
)
print(f"Question: {problem.question}")
print(f"Answer: {problem.answer}")

# Generate a progressive dataset with multiple difficulty levels
problems = generator.generate_progressive_dataset(
    base_size=50,
    include_difficulties=[
        DifficultyLevel.BASIC,
        DifficultyLevel.MEDIUM,
        DifficultyLevel.ADVANCED
    ]
)
```

### Creating Training Datasets

```bash
# Generate training dataset for the NEAT model
python scripts/prepare_training_dataset.py \
    --output_dir ./data/neat_training \
    --general_size 50000 \
    --component_size 10000 \
    --eval_size 10000
```

### End-to-End Training

```bash
# Run the full training pipeline
bash scripts/train_neat_model.sh
```

## Component-Specific Problem Examples

### Titans Memory Test
```
Remember this key-value pair: gamma=42. You will need to recall it later.
```

### Transformer² Adaptation Test
```
Pattern rule: 3→6, 7→14, 10→20. Apply the same rule to find: 15 → ?
```

### Non-Linear Sequence
```
What's the next number in this quadratic sequence: 1, 4, 9, 16?
```

### Multi-Step Problems
```
If you add 5 and 3, then multiply by 2, what do you get?
```

### Algebraic Problems
```
Solve for x: 3x + 5 = 20
```

## Next Steps

1. **Integration with Training Pipeline / BLT/MVOT component Training**
   - Connect the data generation with the main NEAT training infrastructure
   - Implement comprehensive evaluation metrics
   - Download needed datasets to train BLT small lm and download a pretrained visual codebook.
   - Train and 

2. **Component-Specific Evaluation**
   - Create specialized test suites for each NEAT component
   - Implement metrics for measuring component-specific benefits
   - Design controlled experiments for comparative analysis

3. **Training the 100M Parameter Model**
   - Use the generated datasets to train the full NEAT model
   - Fine-tune hyperparameters for stability and performance
   - Evaluate generalization capabilities on out-of-distribution problems